{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adfc141d-5c5a-4ce6-8e6c-a71348508708",
   "metadata": {},
   "source": [
    "# M4 Featurizing with POS for each association and counting the individual appearences of each POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1055c51-e05e-4665-a2c2-14c782e2ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" # gpu usage\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f285ab-a041-4135-a0d3-4c02cbeda030",
   "metadata": {},
   "source": [
    "## POSTable with Amounts of associated lemma included (tokenbased) with NA tag!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1c3f0-a3bf-4a51-ab9a-d2dd36e303e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treetagging in shell and saving output in txt file\n",
    "\n",
    "# DONE IN TreeTagger directory for right working directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fd15ff-3705-4035-a60a-7d3fa56e9154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANZA problem: not good for single word tagging, tags in sentence relations and therefore unknown words are not labelled as X but NN mostly\n",
    "\n",
    "#import tensorflow\n",
    "#import stanza\n",
    "words = ['tokens', 'apple', 'vfkkelwk', 'what', 'laughing', 'bowl']\n",
    "nlp = stanza.Pipeline(model_dir='/users/aylin.wahl/stanza_resources',processors='tokenize, pos',tokenize_pretokenized= True,download_method=None)\n",
    "doc = nlp([words])\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49ce2c8-b73e-4089-92e0-f4e9e50a27d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPACY problem: not running and does not tag words as unknown as well (spacy doe not import due to no menory (restarting hub helps)/ nect problem does not find library)\n",
    "\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "# downgrading spacy\n",
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy==2.3.5\n",
    "!python -m spacy download en_core_web_sm\n",
    "# note: This error originates from a subprocess, and is likely not a problem with pip.ERROR: Failed building wheel for spacy Failed to build spacy ERROR: Could not build wheels for spacy, which is required to install pyproject.toml-based projects /usr/bin/python: No module named spacy\n",
    "\n",
    "#!pip install -U spacy==2.3.5\n",
    "#!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n",
    "\n",
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!pip install resume-parser\n",
    "#!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n",
    "!pip install importlib-metadata==3.2.0\n",
    "\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    doc = nlp(\"He who values value large column sbxdata actual maximum ptsavatar\")\n",
    "\n",
    "    known = set(spell.known([token.text.lower() for token in doc]))\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.lower() in known:\n",
    "            pos = token.pos_\n",
    "        else:\n",
    "            pos = \"\"\n",
    "\n",
    "        print(f'{pos:5s} {token}')     \n",
    "\"\"\"\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "words = ['tokens', 'apple', 'vfkkelwk', 'what', 'laughing', 'bowl']\n",
    "\n",
    "# disable everything except the tagger\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"tagger\"]\n",
    "nlp.disable_pipes(*other_pipes)\n",
    "\n",
    "# use nlp.pipe() instead of nlp() to process multiple texts more efficiently\n",
    "for doc in nlp.pipe(words):\n",
    "    if len(doc) > 0:\n",
    "        print(doc[0].text, doc[0].tag_)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9d302e-049d-4f26-aca9-e23255ee6b26",
   "metadata": {},
   "source": [
    "## POSTable with Amounts of associated lemma included (tokenbased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6b8694-7cf3-47cb-bb6c-d990e2fac07f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # adding row to asscotable with POS information \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger\n",
    "nltk.download('universal_tagset')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#getting the possible tags for brown corpus\n",
    "#nltk.download('tagsets')\n",
    "#nltk.help.brown_tagset()\n",
    "\n",
    "#datatype= 'extreme'\n",
    "datatype= 'filtered'\n",
    "#wordclass = 'adj'\n",
    "wordclass = 'nouns'\n",
    "#wordclass = 'verbs'\n",
    "\n",
    "assoctable = pd.read_csv('/compLing/students/hiwi-theses/data/aylin.wahl/MUDCATData/original_data/targets_Brysbaert_'+datatype+'/noNAtypes_'+wordclass+'_'+datatype+'_AssocTable.csv')\n",
    "\n",
    "'''\n",
    "# POS tagging un known words as Nouns\n",
    "#insert POS row in index position 0 in the assoctable\n",
    "POSrow = []\n",
    "tokens = list(assoctable.columns)\n",
    "pos_tags = nltk.pos_tag(tokens) #PennTreeBank machinelearnaing tagger of-the-shelf\n",
    "for tup,POS in pos_tags:  #pos from tuples into list\n",
    "    POSrow.append(POS)\n",
    "assoctable.loc[-1]=POSrow  #POSlist into dataframe\n",
    "assoctable = assoctable.sort_index().reset_index(drop=True)  #sorting dataframe with POS row\n",
    "assoctable.iloc[0,0]=''  #targets no rowwise POS\n",
    "assoctable.iloc[0,1]=''  #CS no POS\n",
    "'''\n",
    "\n",
    "# POS tagging unknown words as 'None'\n",
    "#insert POS row in index position 0 in the assoctable\n",
    "POSrow = []\n",
    "tokens = list(assoctable.columns)\n",
    "tagger = UnigramTagger(brown.tagged_sents(tagset = 'universal'))   #training data based unigram tagging, with brown corpus trained tagset='universal'\n",
    "for word, tag in tagger.tag(tokens):\n",
    "    POSrow.append(tag)\n",
    "assoctable.loc[-1]=POSrow  #POSlist into dataframe\n",
    "assoctable = assoctable.sort_index().reset_index(drop=True)  #sorting dataframe with POS row\n",
    "assoctable.loc[0] =  assoctable.loc[0].replace([None], 'None')# making None POS to a string\n",
    "assoctable.iloc[0,0]=''  #targets no rowwise POS\n",
    "assoctable.iloc[0,1]=''  #CS no POS\n",
    "# too many unknown tags?\n",
    "\n",
    "assoctable.to_csv('/compLing/students/hiwi-theses/data/aylin.wahl/MUDCATData/M4_POStable/NoUnknownNoNAtypes_'+wordclass+'_'+datatype+'_POStableAssoccount.csv',index=False)   \n",
    "# counting the amount of None tagged associations (without token count) and listing them to get an insight which words are missing\n",
    "\n",
    "print('All Associations: '+str(len(list(assoctable.loc[0]))-2)) # all associations\n",
    "print('None Associations: '+str(list(assoctable.loc[0]).count('None'))) #count of None associations\n",
    "none = list(assoctable.apply(lambda row: row[row == 'None'].index, axis=1))  # list of associations that are None tagged\n",
    "nonelist = []\n",
    "for x in range(len(none[0])):\n",
    "    nonelist.append(none[0][x])\n",
    "with open('/compLing/students/hiwi-theses/projects/aylin.wahl/M4/'+wordclass+'_NoneAssociations.txt', \"a\") as f:\n",
    "    print('All Associations: '+str(len(list(assoctable.loc[0]))-2),file=f) # all associations\n",
    "    print('None Associations: '+str(list(assoctable.loc[0]).count('None')),file=f)\n",
    "    for assoc in nonelist:\n",
    "        print(assoc, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98465789-d0f7-4c62-9585-a703cdea33db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CS</th>\n",
       "      <th>.</th>\n",
       "      <th>DET</th>\n",
       "      <th>ADV</th>\n",
       "      <th>ADP</th>\n",
       "      <th>PRON</th>\n",
       "      <th>X</th>\n",
       "      <th>None</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>PRT</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>NUM</th>\n",
       "      <th>NaN</th>\n",
       "      <th>VERB</th>\n",
       "      <th>NaN</th>\n",
       "      <th>CONJ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.34</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>171</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.59</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>76</td>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.64</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>209</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3694</th>\n",
       "      <td>2.11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3695</th>\n",
       "      <td>4.59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696</th>\n",
       "      <td>4.20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3697</th>\n",
       "      <td>4.67</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>223</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3698</th>\n",
       "      <td>3.04</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3698 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CS  .  DET  ADV  ADP  PRON  X  None  NOUN  PRT  ADJ  NUM  NaN  VERB  \\\n",
       "1     4.31  0    0    1    0     0  0    21   216    0   22    0    0    31   \n",
       "2     2.34  0   13   18    8     0  0    21   171   20   16    3    0    16   \n",
       "3     2.59  0   13   10    4     0  3    51    76    5   51   49    0    20   \n",
       "4     4.93  0    0    5    2     0  0    29   178    0   21    0    0    58   \n",
       "5     4.64  0    3    0    2     0  0    31   209    8   30    0    0     6   \n",
       "...    ... ..  ...  ...  ...   ... ..   ...   ...  ...  ...  ...  ...   ...   \n",
       "3694  2.11  0    0    4    0     0  0    44   152    0   68    2    0     5   \n",
       "3695  4.59  0    0    0    1     0  0    46   171    0   16    0    0    13   \n",
       "3696  4.20  0    0    1    3     0  0    47   178    0    9    1    0     5   \n",
       "3697  4.67  0    1    1    0     0  0    38   223    0   14    0    0    13   \n",
       "3698  3.04  0    1    0    0     0  0    29   110    0   90    0    0     3   \n",
       "\n",
       "      NaN  CONJ  \n",
       "1       0     1  \n",
       "2       0     0  \n",
       "3       0     1  \n",
       "4       0     0  \n",
       "5       0     0  \n",
       "...   ...   ...  \n",
       "3694    0     0  \n",
       "3695    0     0  \n",
       "3696    0     0  \n",
       "3697    0     0  \n",
       "3698    0     0  \n",
       "\n",
       "[3698 rows x 16 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POStable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ca4719-8886-406a-bd7e-ea90fa4fd679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      282.25\n",
      "2      288.56\n",
      "3      287.72\n",
      "4      277.38\n",
      "5      283.68\n",
      "        ...  \n",
      "765    226.68\n",
      "766    222.33\n",
      "767    256.17\n",
      "768    253.64\n",
      "769    267.55\n",
      "Length: 769, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# counting POStags for each target (takes forever!!!!!!) only needed once\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "#wordclass = 'adj'\n",
    "#wordclass = 'nouns'\n",
    "wordclass = 'verbs'\n",
    "\n",
    "assoctable = pd.read_csv('/compLing/students/hiwi-theses/data/aylin.wahl/MUDCATData/M4_POStable/NoUnknownNoNAtypes_'+wordclass+'_filtered_POStableAssoccount.csv', low_memory=False)\n",
    "\n",
    "#countsALL = Counter( tag for word,  tag in pos_tags) #counting tags for whole asssoctable\n",
    "#POStagsALL = list(countsALL)         #list of tags for all assoc in assoctable\n",
    "# gettig all POS tags used in this process from the  brown tagger (browntagset)\n",
    "POStagsALL = list(set(list(assoctable.loc[0])))\n",
    "POStable = pd.DataFrame(data=assoctable['CS'], index=list(assoctable.index), columns=['CS']+POStagsALL)\n",
    "POStable = POStable.replace([None], 0)\n",
    "POStable = POStable.drop(0,axis=0)\n",
    "\n",
    "assoctable.iloc[1:,1:] = assoctable.iloc[1:,1:].astype(int)\n",
    "associationlist = list(assoctable.iloc[1:,2:].columns)\n",
    "# counting the POS amount per target with tokenamount\n",
    "for index, row in assoctable.iloc[1:,2:].iterrows():\n",
    "    indexesnonzero = np.nonzero(list(row))  #getting the indexes of the nonzero values to count them individually\n",
    "    for ind in range(0,len(list(indexesnonzero)[0])):\n",
    "        i = list(indexesnonzero)[0][ind] #index of the column with nonzero value\n",
    "        column = associationlist[i]      #getting the association with a nonzero value\n",
    "        pos = assoctable.loc[0,column]   # getting POS of the word\n",
    "        amount = assoctable.iloc[1:,2:].loc[index,column]# getting the amount to add to the postable\n",
    "        POStable.loc[index,pos] += amount\n",
    "          \n",
    "POStable.to_csv('/compLing/students/hiwi-theses/data/aylin.wahl/MUDCATData/M4_POStable/NoUnknownNoNAtypes_'+wordclass+'_filtered_POStable.csv',index=False)   \n",
    "\n",
    "print(POStable.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f3a9d-3821-470f-b3e9-60eb3f435476",
   "metadata": {},
   "source": [
    "## POStable without Amounts of associated lemmas (typebased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca0005-5e08-488a-9896-5cdf387dc336",
   "metadata": {},
   "outputs": [],
   "source": [
    "POStable.to_csv('/compLing/students/hiwi-theses/data/aylin.wahl/MUDCATData/M4_POStable/NoUnknownNoNAtypes_'+wordclass+'_filtered_POStable.csv',index=False)   \n",
    "\n",
    "print(POStable.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a05a18-177a-411f-a7fe-693ff7e423f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "POStable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5075097-aa99-4439-a7f7-a62349211630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POStagging all associations and counting the N/V/A/Other in the associated lemmas for a target\n",
    "\n",
    " # adding row to asscotable with POS information \n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#datatype= 'extreme'\n",
    "datatype= 'filtered'\n",
    "#wordclass = 'adj'\n",
    "wordclass = 'nouns'\n",
    "#wordclass = 'verbs'\n",
    "\n",
    "assoctable = pd.read_csv('/compLing/students/hiwi-theses/data/aylin.wahl/MUDCATData/original_data/targets_Brysbaert_'+datatype+'/noNAtypes_'+wordclass+'_'+datatype+'_AssocTable.csv')\n",
    "\n",
    "# POS tagging un known words as Nouns\n",
    "#insert POS row in index position 0 in the assoctable\n",
    "POSrow = []\n",
    "tokens = list(assoctable.columns)\n",
    "pos_tags = nltk.pos_tag(tokens) #PennTreeBank machinelearnaing tagger of-the-shelf\n",
    "for tup,POS in pos_tags:  #pos from tuples into list\n",
    "    POSrow.append(POS)\n",
    "assoctable.loc[-1]=POSrow  #POSlist into dataframe\n",
    "assoctable = assoctable.sort_index().reset_index(drop=True)  #sorting dataframe with POS row\n",
    "assoctable.iloc[0,0]=''  #targets no rowwise POS\n",
    "assoctable.iloc[0,1]=''  #CS no POS\n",
    "\n",
    "#setting up POStable\n",
    "from collections import Counter\n",
    "\n",
    "countsALL = Counter( tag for word,  tag in pos_tags) #counting tags for whole asssoctable\n",
    "POStagsALL = list(countsALL)         #list of tags for all assoc in assoctable\n",
    "POStable = pd.DataFrame(index=list(assoctable.index), columns=['Unnamed: 0','CS']+POStagsALL)\n",
    "POStable['CS'] = assoctable['CS']\n",
    "POStable['Unnamed: 0'] = assoctable['Unnamed: 0']\n",
    "POStable = POStable.replace([None], 0)\n",
    "POStable = POStable.drop([0],axis=0)\n",
    "\n",
    "#filling POStable dataframe with counts of the POS\n",
    "for ind in range(1,len(assoctable.index)):        \n",
    "    #target = list(assoctable.iloc[ind])\n",
    "    nonzeros = assoctable.iloc[ind].to_numpy().nonzero()  #getting a list of the indexes of the nonzero values\n",
    "    nonzeros = nonzeros[0].tolist()\n",
    "    nonzeros.remove(0)# deleting 0 and 1 value for targetname and CS\n",
    "    nonzeros.remove(1)\n",
    "    for i in nonzeros:\n",
    "        #non = nonzeros[i]\n",
    "        POS = assoctable[assoctable.columns[i]][0] #getting POS of nonzero association\n",
    "        POStable.loc[ind,POS] += 1 # counting +1 for this POS in the target row of POStable\n",
    "\n",
    "POStable.to_csv('/compLing/students/hiwi-theses/data/aylin.wahl/MUDCATData/M4_POStable/noNAtypes_'+wordclass+'_'+datatype+'_POStableLemmacount.csv',index=False)   \n",
    "\n",
    "print(POStable.sum(axis=1))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656bf8a7-67c7-4516-838b-9bb70b4e776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "POStable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f33774e-12ce-4b84-9ffa-f87b90af1288",
   "metadata": {},
   "source": [
    "# Normalizing POStable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3afadd-d494-44e7-9028-3863dde334f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 normalization by dividing amount of associated POS by amount of all associated POS\n",
    "import pandas as pd\n",
    "\n",
    "wordclass = 'nouns'\n",
    "#wordclass = 'adj'\n",
    "#wordclass = 'verbs'\n",
    "\n",
    "df = pd.read_csv('/compLing/students/hiwi-theses/data/aylin.wahl/MUDCATData/M4_POStable/noNAtypes_'+wordclass+'_filtered_POStable.csv')\n",
    "df = df.drop(df.index[0]) #delete whole amount of POS association row\n",
    "df.reset_index(inplace=True) #reset index\n",
    "cs = df['CS']\n",
    "df = df.drop(['CS'],axis=1) #drop CS to exclude from normalization\n",
    "\n",
    "df = df.div(df.sum(axis=1),axis=0)\n",
    "df.insert(0, 'CS', cs) #adding back Concreteness scores\n",
    "t = pd.read_csv('/compLing/students/hiwi-theses/data/aylin.wahl/MUDCATData/original_data/targets_Brysbaert_filtered/noNAtypes_'+wordclass+'_filtered_AssocTable.csv')\n",
    "df.insert(0,'Unnamed: 0', t['Unnamed: 0']) #adding targets whihc got lost through POS counting\n",
    "\n",
    "df.to_csv('/compLing/students/hiwi-theses/data/aylin.wahl/MUDCATData/M4_POStable/NORMnoNAtypes_'+wordclass+'_filtered_POStable.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c79b5d-e2e5-486f-8f9a-f8cb07a75e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
